{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for classifying letters \n",
    "By: Hesham Asem\n",
    "\n",
    "______\n",
    "\n",
    "here we'll build a Conv2d to be used in reading & classifying about half million pictures of  first 10 alphabetic letters . . \n",
    "\n",
    "you can find data file  here :  https://www.kaggle.com/jwjohnson314/notmnist\n",
    "\n",
    "let;s first import libraries \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import os\n",
    "import glob as gb\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten ,Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we need to check the folders to know what letters available . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 10 letters , which are : ['A', 'D', 'I', 'G', 'H', 'B', 'F', 'C', 'J', 'E']\n"
     ]
    }
   ],
   "source": [
    "all_letters = os.listdir('../input/notmnist/notMNIST_large/notMNIST_large')\n",
    "\n",
    "print(f'We have {len(all_letters)} letters , which are : {all_letters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "\n",
    "# Read Data . \n",
    "\n",
    "we'll use glob library to collect all png pictures to know how many pictures we have for each letter . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for letter A we have  52912 available images\n",
      "for letter D we have  52912 available images\n",
      "for letter I we have  52912 available images\n",
      "for letter G we have  52912 available images\n",
      "for letter H we have  52912 available images\n",
      "for letter B we have  52912 available images\n",
      "for letter F we have  52912 available images\n",
      "for letter C we have  52912 available images\n",
      "for letter J we have  52911 available images\n",
      "for letter E we have  52912 available images\n",
      "-----------------------\n",
      "Total Images are 529119 images\n"
     ]
    }
   ],
   "source": [
    "total_images = 0\n",
    "for letter in all_letters : \n",
    "    available_images = gb.glob(pathname= f'../input/notmnist/notMNIST_large/notMNIST_large/{letter}/*.png')\n",
    "    total_images+=len(available_images)\n",
    "    print(f'for letter {letter} we have  {len(available_images)} available images')\n",
    "print('-----------------------')    \n",
    "print(f'Total Images are {total_images} images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total 529 thousand images for all 10 letters , now let's create X & y variables , so we can fill them with read data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(np.zeros(shape=(total_images , 28,28)))\n",
    "y = list(np.zeros(shape=(total_images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now to open each file & read it using plt.imread , then fill it in its place in X & y data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "y_value = 0\n",
    "for letter in all_letters : \n",
    "    available_images = gb.glob(pathname= f'../input/notmnist/notMNIST_large/notMNIST_large/{letter}/*.png')\n",
    "    for image in available_images : \n",
    "        try : \n",
    "            x = plt.imread(image)\n",
    "            X[i] = x\n",
    "            y[i] = y_value\n",
    "            i+=1\n",
    "        except : \n",
    "            pass\n",
    "    y_value+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# Forming Dimensions\n",
    "\n",
    "since (y) data now is a single number vary from 0 to 9 , we'll need to categorize it using OneHotEncoder from sklearn , so it be ready for the softmax activation functing in CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "ohe  = OneHotEncoder()\n",
    "y = np.array(y)\n",
    "y = y.reshape(len(y), 1)\n",
    "ohe.fit(y)\n",
    "y = ohe.transform(y).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can check a random y value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we have to expand X dimension to be suitable with the CNN dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.expand_dims(X, -1).astype('float32')/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now X shape should be : sample size * 28 * 28 * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529119, 28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data\n",
    "\n",
    "now lets split our dat to Train , Cross-Validation & Test sets . . \n",
    "\n",
    "first to create X_part & y_part which is 85% of data , also X_test & y_test which is 15%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "cd155465-ef62-4af0-a01a-13174c3b6049",
    "_uuid": "88d7b01f5501f57ce762d8a63dc238d97571b013",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape is  (449751, 28, 28, 1)\n",
      "X_test shape is  (79368, 28, 28, 1)\n",
      "y_train shape is  (449751, 10)\n",
      "y_test shape is  (79368, 10)\n"
     ]
    }
   ],
   "source": [
    "X_part, X_cv, y_part, y_cv = train_test_split(X, y, test_size=0.15, random_state=44, shuffle =True)\n",
    "\n",
    "print('X_train shape is ' , X_part.shape)\n",
    "print('X_test shape is ' , X_cv.shape)\n",
    "print('y_train shape is ' , y_part.shape)\n",
    "print('y_test shape is ' , y_cv.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then to split X_part & y_part into train & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape is  (337313, 28, 28, 1)\n",
      "X_test shape is  (112438, 28, 28, 1)\n",
      "y_train shape is  (337313, 10)\n",
      "y_test shape is  (112438, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_part, y_part, test_size=0.25, random_state=44, shuffle =True)\n",
    "\n",
    "print('X_train shape is ' , X_train.shape)\n",
    "print('X_test shape is ' , X_test.shape)\n",
    "print('y_train shape is ' , y_train.shape)\n",
    "print('y_test shape is ' , y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "now let's build the model with Keras , using Conv2d & Maxpooling tools , & not to forget to dropout some cells to avoid OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "KerasModel = keras.models.Sequential([\n",
    "        keras.layers.Conv2D(filters = 32, kernel_size = 4,  activation = tf.nn.relu , padding = 'same'),\n",
    "        keras.layers.MaxPool2D(pool_size=(3,3), strides=None, padding='valid'),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(filters=32, kernel_size=4,activation = tf.nn.relu , padding='same'),\n",
    "        keras.layers.MaxPool2D(),\n",
    "        keras.layers.BatchNormalization(),\n",
    "        keras.layers.Conv2D(filters=64, kernel_size=5,activation = tf.nn.relu , padding='same'),\n",
    "        keras.layers.MaxPool2D(),\n",
    "        keras.layers.Flatten(),    \n",
    "        keras.layers.Dropout(0.5),        \n",
    "        keras.layers.Dense(64),    \n",
    "        keras.layers.Dropout(0.3),            \n",
    "        keras.layers.Dense(units= 10,activation = tf.nn.softmax ),                \n",
    "\n",
    "    ])\n",
    "    \n",
    "\n",
    "KerasModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then to train it , using few number of epochs to avoid OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "7c234ad9-e48e-45ba-b588-0cdb1f74d870",
    "_uuid": "c6dc0f959d4b554c0aeec6b67ea43bfa40e45308",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 337313 samples, validate on 79368 samples\n",
      "Epoch 1/3\n",
      "337313/337313 [==============================] - 42s 125us/step - loss: 0.4316 - acc: 0.8713 - val_loss: 0.3589 - val_acc: 0.8976\n",
      "Epoch 2/3\n",
      "337313/337313 [==============================] - 39s 115us/step - loss: 0.3442 - acc: 0.8969 - val_loss: 0.3090 - val_acc: 0.9096\n",
      "Epoch 3/3\n",
      "337313/337313 [==============================] - 38s 113us/step - loss: 0.3191 - acc: 0.9034 - val_loss: 0.2927 - val_acc: 0.9112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6e03459048>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train\n",
    "KerasModel.fit(X_train,y_train,validation_data=(X_cv, y_cv),epochs=3,batch_size=64,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now how the model looks like ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        544       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 9, 9, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 9, 9, 32)          16416     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 32)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 4, 4, 64)          51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 2, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 85,578\n",
      "Trainable params: 85,450\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "KerasModel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting\n",
    "\n",
    "then we'll predict X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Shape is (112438, 10)\n"
     ]
    }
   ],
   "source": [
    "y_pred = KerasModel.predict(X_test)\n",
    "\n",
    "print('Prediction Shape is {}'.format(y_pred.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can check random samples from X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for sample  98096  the predicted value is   F   , while the actual letter is F\n",
      "for sample  103877  the predicted value is   I   , while the actual letter is H\n",
      "for sample  10249  the predicted value is   D   , while the actual letter is D\n",
      "for sample  97628  the predicted value is   H   , while the actual letter is H\n",
      "for sample  95360  the predicted value is   C   , while the actual letter is C\n",
      "for sample  41433  the predicted value is   D   , while the actual letter is D\n",
      "for sample  74535  the predicted value is   C   , while the actual letter is C\n",
      "for sample  68969  the predicted value is   F   , while the actual letter is F\n",
      "for sample  108700  the predicted value is   B   , while the actual letter is B\n",
      "for sample  81442  the predicted value is   C   , while the actual letter is I\n"
     ]
    }
   ],
   "source": [
    "Letters ={0:'A', 1:'B' , 2:'C' ,3:'D' ,4:'E' ,5:'F' ,6:'G' ,7:'H' ,8:'I' ,9:'J' }\n",
    "\n",
    "for i in list(np.random.randint(0,len(X_test) ,size= 10)) : \n",
    "    print(f'for sample  {i}  the predicted value is   {Letters[np.argmax(y_pred[i])]}   , while the actual letter is {Letters[np.argmax(y_test[i])]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "& to measure the loss & accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112438/112438 [==============================] - 7s 58us/step\n",
      "Test Loss is 0.2878474014839032\n",
      "Test Accuracy is 0.9132766502417402\n"
     ]
    }
   ],
   "source": [
    "ModelLoss, ModelAccuracy = KerasModel.evaluate(X_test, y_test)\n",
    "\n",
    "print('Test Loss is {}'.format(ModelLoss))\n",
    "print('Test Accuracy is {}'.format(ModelAccuracy ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "great , with 91% accuracy we achieved good result without moving to OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
